{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_data_reader import get_datasets_pytorch, prep_data_range, move_alt_axis_for_numpy\n",
    "from utils_plot import plot_rho_24, get_rho_at_date\n",
    "from ipynb.fs.full.model import Encoder, Decoder, Autoencoder # type: ignore\n",
    "from utils_optim import CustomLoss, EarlyStopping, numpy_custom_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "This file is written to save .npy data for the outpus of NN. The .npy will be used in the next script. hw1_c_dicussion.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Loading data for year 2011\n",
      "Loading data for year 2012\n",
      "Loading data for year 2013\n",
      "Loading data for year 2014\n",
      "Loading data for year 2015\n",
      "Loading data for year 2016\n",
      "Concatenating data...\n",
      "Preparing test data...\n",
      "Loading data for year 2017\n",
      "Concatenating data...\n",
      "Train Set Shape: (41972, 36, 20, 24)\n",
      "Val Set Shape:  (10492, 36, 20, 24)\n",
      "Test Set Shape: (8736, 36, 20, 24)\n"
     ]
    }
   ],
   "source": [
    "# Pytorch's Dataset objects. By default, reading in log10\n",
    "train, val, test, original_trainset= get_datasets_pytorch(train_ds=[2011, 2012, 2013, 2014, 2015, 2016], test_ds=[2017], train_ratio=0.8, shuffle=True)\n",
    "\n",
    "\n",
    "class StandardScaler():\n",
    "    def __init__(self, data) -> None:\n",
    "        self.mean = np.mean(data)\n",
    "        self.std = np.std(data)\n",
    "    \n",
    "    def transform(self, data):\n",
    "        data = (data - self.mean) / self.std\n",
    "        self.min = np.min(data)\n",
    "        self.max = np.max(data)\n",
    "        data = (data - self.min) / (self.max - self.min)\n",
    "        return data\n",
    "    \n",
    "    def inverse_transform(self, data):\n",
    "        data = data * (self.max - self.min) + self.min\n",
    "        data = data * self.std + self.mean\n",
    "        return data\n",
    "\n",
    "ss = StandardScaler(original_trainset)\n",
    "\n",
    "# # # Now, all NN's input is scaled to [0, 1]\n",
    "# train.data = ss.transform(train.data)\n",
    "# val.data = ss.transform(val.data)\n",
    "test.data = ss.transform(test.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0011\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create instances of the encoder and decoder for this cell in the nb.\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "autoencoder = Autoencoder(encoder, decoder)\n",
    "\n",
    "# double precision\n",
    "double_precision = False\n",
    "\n",
    "if double_precision:\n",
    "    autoencoder = autoencoder.double()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = CustomLoss(alpha=0.9995122197766073, epsillon=1e-6)\n",
    "\n",
    "# instantiate the model\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "autoencoder = Autoencoder(encoder, decoder)\n",
    "# test now\n",
    "\n",
    "testLoader = DataLoader(test, batch_size=64)\n",
    "# load weight from the last\n",
    "autoencoder.load_state_dict(torch.load(f'./weights/autoencoder_best3.pth'))\n",
    "nn.DataParallel(autoencoder).to(device)\n",
    "\n",
    "\n",
    "autoencoder.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "        inputs = data.to(device) if double_precision else data.float().to(device)\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    test_loss = test_loss / len(testLoader.dataset)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        \n",
    "# get all outputs as numpy\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "        inputs = data.to(device) if double_precision else data.float().to(device)\n",
    "        outputs.append(autoencoder(inputs).cpu().numpy())\n",
    "\n",
    "nn_prediction = np.concatenate(outputs, axis=0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in NN's distribution\n",
      "MSE: 0.0011, MAPE: 0.0902, Total: 0.0100\n",
      "Loss in original distribution\n",
      "MSE: 15864.6699, MAPE: 641.6058, Total: 14342.3635\n",
      "(8736, 24, 20, 36)\n"
     ]
    }
   ],
   "source": [
    "# show loss in NN's ditribution\n",
    "mse, mape, total = numpy_custom_loss(nn_prediction, test.data)\n",
    "np.save('pr_2017_nn.npy', nn_prediction)\n",
    "np.save('gt_2017_nn.npy', test.data)\n",
    "print(\"Loss in NN's distribution\")\n",
    "print(f\"MSE: {mse:.4f}, MAPE: {mape:.4f}, Total: {total:.4f}\")\n",
    "\n",
    "# inverse transform to the original distrubiton and scale\n",
    "pr_2017 = ss.inverse_transform(nn_prediction)\n",
    "mse, mape, total = numpy_custom_loss(nn_prediction, ss.inverse_transform(pr_2017))\n",
    "print(\"Loss in original distribution\")\n",
    "print(f\"MSE: {mse:.4f}, MAPE: {mape:.4f}, Total: {total:.4f}\")\n",
    "\n",
    "# for original dimensions\n",
    "pr_2017_orignal_dim = move_alt_axis_for_numpy(nn_prediction)\n",
    "print(pr_2017_orignal_dim.shape)\n",
    "\n",
    "# # save the npy\n",
    "# np.save('pr_2017.npy', pr_2017_orignal_dim)\n",
    "# print(f\"saved pr_2017.npy\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
